#!/usr/bin/env python
#
# Cloudlet Infrastructure for Mobile Computing
#
#   Author: Kiryong Ha <krha@cmu.edu>
#
#   Copyright (C) 2011-2013 Carnegie Mellon University
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#

import os
import functools
import traceback
import sys
import time
import struct
import Queue
import SocketServer
import socket
import subprocess

import tempfile
import multiprocessing
import threading
from optparse import OptionParser
from hashlib import sha256

if os.path.exists("../elijah") is True:
    sys.path.insert(0, "../")
from elijah.provisioning.synthesis import validate_congifuration
from elijah.provisioning import delta
from elijah.provisioning.delta import DeltaItem
from elijah.provisioning.server import NetworkUtil
from elijah.provisioning.synthesis_protocol import Protocol as Protocol
from elijah.provisioning.synthesis import run_fuse
from elijah.provisioning.synthesis import SynthesizedVM
from elijah.provisioning.synthesis import connect_vnc
from elijah.provisioning.handoff import HandoffDataRecv

#import synthesis as synthesis
#from package import VMOverlayPackage
from elijah.provisioning.db.api import DBConnector
from elijah.provisioning.db.table_def import BaseVM
from elijah.provisioning.configuration import Const as Cloudlet_Const
from elijah.provisioning.compression import DecompProc
from elijah.provisioning import tool
from elijah.provisioning import log as logging

import mmap
from pprint import pformat


LOG = logging.getLogger(__name__)
session_resources = dict()   # dict[session_id] = obj(SessionResource)


def wrap_process_fault(function):
    """Wraps a method to catch exceptions related to instances.
    This decorator wraps a method to catch any exceptions and
    terminate the request gracefully.
    """
    @functools.wraps(function)
    def decorated_function(self, *args, **kwargs):
        try:
            return function(self, *args, **kwargs)
        except Exception as e:
            if hasattr(self, 'exception_handler'):
                self.exception_handler()
            kwargs.update(dict(zip(function.func_code.co_varnames[2:], args)))
            LOG.error("failed with : %s" % str(kwargs))

    return decorated_function


def try_except(fn):
    def wrapped(*args, **kwargs):
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            et, ei, tb = sys.exc_info()
            raise MyError, MyError(e), tb
    return wrapped


class StreamSynthesisError(Exception):
    pass


class AckThread(threading.Thread):

    def __init__(self, request):
        self.request = request
        self.ack_queue = Queue.Queue()
        threading.Thread.__init__(self, target=self.start_sending_ack)

    def start_sending_ack(self):
        while True:
            data = self.ack_queue.get()
            bytes_recved = self.ack_queue.get()
            # send ack
            ack_data = struct.pack("!Q", bytes_recved)
            self.request.sendall(ack_data)

    def signal_ack(self):
        self.ack_queue.put(bytes_recved)


class RecoverDeltaProc(multiprocessing.Process):
    FUSE_INDEX_DISK = 1
    FUSE_INDEX_MEMORY = 2

    def __init__(self, base_disk, base_mem,
                 decomp_delta_queue, output_mem_path,
                 output_disk_path, chunk_size,
                 fuse_info_queue):
        if base_disk is None and base_mem is None:
            raise StreamSynthesisError("Need either base_disk or base_memory")

        self.decomp_delta_queue = decomp_delta_queue
        self.output_mem_path = output_mem_path
        self.output_disk_path = output_disk_path
        self.fuse_info_queue = fuse_info_queue
        self.base_disk = base_disk
        self.base_mem = base_mem

        self.base_disk_fd = None
        self.base_mem_fd = None
        self.raw_disk = None
        self.raw_mem = None
        self.mem_overlay_dict = None
        self.raw_mem_overlay = None
        self.chunk_size = chunk_size
        self.zero_data = struct.pack("!s", chr(0x00)) * chunk_size
        self.recovered_delta_dict = dict()
        self.recovered_hash_dict = dict()
        self.live_migration_iteration_dict = dict()

        multiprocessing.Process.__init__(self, target=self.recover_deltaitem)

    def recover_deltaitem(self):
        time_start = time.time()

        # initialize reference data to use mmap
        count = 0
        self.base_disk_fd = open(self.base_disk, "rb")
        self.raw_disk = mmap.mmap(
            self.base_disk_fd.fileno(),
            0, prot=mmap.PROT_READ)
        self.base_mem_fd = open(self.base_mem, "rb")
        self.raw_mem = mmap.mmap(
            self.base_mem_fd.fileno(),
            0, prot=mmap.PROT_READ)
        self.recover_mem_fd = open(self.output_mem_path, "wrb")
        self.recover_disk_fd = open(self.output_disk_path, "wrb")

        unresolved_deltaitem_list = []
        while True:
            recv_data = self.decomp_delta_queue.get()
            if recv_data == Cloudlet_Const.QUEUE_SUCCESS_MESSAGE:
                break

            overlay_chunk_ids = list()
            # recv_data is a single blob so that it contains whole DeltaItem
            LOG.debug("%f\trecover one blob" % (time.time()))
            delta_item_list = RecoverDeltaProc.from_buffer(recv_data)
            for delta_item in delta_item_list:
                ret = self.recover_item(delta_item)
                if ret is None:
                    # cannot find self reference point due to the parallel
                    # compression. Save this and do it later
                    unresolved_deltaitem_list.append(delta_item)
                    continue
                self.process_deltaitem(delta_item, overlay_chunk_ids)
                count += 1

            self.recover_mem_fd.flush()
            self.recover_disk_fd.flush()

        LOG.info(
            "[Delta] Handle dangling DeltaItem (%d)" %
            len(unresolved_deltaitem_list))
        overlay_chunk_ids = list()
        for delta_item in unresolved_deltaitem_list:
            ret = self.recover_item(delta_item)
            if ret is None:
                msg = "Cannot find self reference: type(%ld), offset(%ld), index(%ld)" % (
                    delta_item.delta_type, delta_item.offset, delta_item.index)
                raise StreamSynthesisError(msg)
            self.process_deltaitem(delta_item, overlay_chunk_ids)
            count += 1

        self.recover_mem_fd.close()
        self.recover_mem_fd = None
        self.recover_disk_fd.close()
        self.recover_disk_fd = None
        time_end = time.time()

        LOG.info("[time] Delta delta %ld chunks, (%s~%s): %s" %
                (count, time_start, time_end, (time_end-time_start)))
        LOG.info("Finish VM handoff")

    def recover_item(self, delta_item):
        if not isinstance(delta_item, DeltaItem):
            raise StreamSynthesisError("Need list of DeltaItem")

        if (delta_item.ref_id == DeltaItem.REF_RAW):
            recover_data = delta_item.data
        elif (delta_item.ref_id == DeltaItem.REF_ZEROS):
            recover_data = self.zero_data
        elif (delta_item.ref_id == DeltaItem.REF_BASE_MEM):
            offset = delta_item.data
            recover_data = self.raw_mem[offset:offset+self.chunk_size]
        elif (delta_item.ref_id == DeltaItem.REF_BASE_DISK):
            offset = delta_item.data
            recover_data = self.raw_disk[offset:offset+self.chunk_size]
        elif delta_item.ref_id == DeltaItem.REF_SELF:
            ref_index = delta_item.data
            self_ref_delta_item = self.recovered_delta_dict.get(
                ref_index, None)
            if self_ref_delta_item is None:
                return None
            recover_data = self_ref_delta_item.data
        elif delta_item.ref_id == DeltaItem.REF_SELF_HASH:
            ref_hashvalue = delta_item.data
            self_ref_delta_item = self.recovered_hash_dict.get(
                ref_hashvalue, None)
            if self_ref_delta_item is None:
                return None
            recover_data = self_ref_delta_item.data
            delta_item.hash_value = ref_hashvalue
        elif delta_item.ref_id == DeltaItem.REF_XDELTA:
            patch_data = delta_item.data
            patch_original_size = delta_item.offset_len
            if delta_item.delta_type == DeltaItem.DELTA_MEMORY or\
                    delta_item.delta_type == DeltaItem.DELTA_MEMORY_LIVE:
                base_data = self.raw_mem[
                    delta_item.offset:delta_item.offset + patch_original_size]
            elif delta_item.delta_type == DeltaItem.DELTA_DISK or\
                    delta_item.delta_type == DeltaItem.DELTA_DISK_LIVE:
                base_data = self.raw_disk[
                    delta_item.offset:delta_item.offset + patch_original_size]
            else:
                raise StreamSynthesisError(
                    "Delta type should be either disk or memory")
            recover_data = tool.merge_data(
                base_data, patch_data,
                len(base_data) * 5)
        elif delta_item.ref_id == DeltaItem.REF_BSDIFF:
            patch_data = delta_item.data
            patch_original_size = delta_item.offset_len
            if delta_item.delta_type == DeltaItem.DELTA_MEMORY or\
                    delta_item.delta_type == DeltaItem.DELTA_MEMORY_LIVE:
                base_data = self.raw_mem[
                    delta_item.offset:delta_item.offset + patch_original_size]
            elif delta_item.delta_type == DeltaItem.DELTA_DISK or\
                    delta_item.delta_type == DeltaItem.DELTA_DISK_LIVE:
                base_data = self.raw_disk[
                    delta_item.offset:delta_item.offset + patch_original_size]
            else:
                raise DeltaError("Delta type should be either disk or memory")
            recover_data = tool.merge_data_bsdiff(base_data, patch_data)
        elif delta_item.ref_id == DeltaItem.REF_XOR:
            patch_data = delta_item.data
            patch_original_size = delta_item.offset_len
            if delta_item.delta_type == DeltaItem.DELTA_MEMORY or\
                    delta_item.delta_type == DeltaItem.DELTA_MEMORY_LIVE:
                base_data = self.raw_mem[
                    delta_item.offset:delta_item.offset + patch_original_size]
            elif delta_item.delta_type == DeltaItem.DELTA_DISK or\
                    delta_item.delta_type == DeltaItem.DELTA_DISK_LIVE:
                base_data = self.raw_disk[
                    delta_item.offset:delta_item.offset + patch_original_size]
            else:
                raise DeltaError("Delta type should be either disk or memory")
            recover_data = tool.cython_xor(base_data, patch_data)
        else:
            raise StreamSynthesisError(
                "Cannot recover: invalid referce id %d" %
                delta_item.ref_id)

        if len(recover_data) != delta_item.offset_len:
            msg = "Error, Recovered Size Error: %d, %d, ref_id: %s, data_len: %ld, offset: %ld, offset_len: %ld" % \
                (delta_item.delta_type, len(recover_data), delta_item.ref_id,
                 delta_item.data_len, delta_item.offset, delta_item.offset_len)
            print msg
            raise StreamSynthesisError(msg)

        # recover
        delta_item.ref_id = DeltaItem.REF_RAW
        delta_item.data = recover_data
        if delta_item.hash_value is None or len(delta_item.hash_value) == 0:
            delta_item.hash_value = sha256(recover_data).digest()

        return delta_item

    @staticmethod
    def from_buffer(data):
        #import yappi
        # yappi.start()
        cur_offset = 0
        deltaitem_list = list()
        while True:
            new_item, offset = RecoverDeltaProc.unpack_stream(
                data[cur_offset:])
            cur_offset += offset
            if len(data) < cur_offset:
                break
            deltaitem_list.append(new_item)
        # yappi.get_func_stats().print_all()
        return deltaitem_list

    @staticmethod
    def unpack_stream(stream, with_hashvalue=False):
        if len(stream) == 0:
            return None, 999999
        offset = 0
        live_seq = None
        data = stream[0:8+2+1]
        data_len = 0
        offset += (8+2+1)
        (ram_offset, offset_len, ref_info) = struct.unpack("!QHc", data)
        ref_id = ord(ref_info) & 0xF0
        delta_type = ord(ref_info) & 0x0F

        if ref_id == DeltaItem.REF_RAW or \
                ref_id == DeltaItem.REF_XDELTA or \
                ref_id == DeltaItem.REF_XOR or \
                ref_id == DeltaItem.REF_BSDIFF:
            data_len = struct.unpack("!Q", stream[offset:offset+8])[0]
            offset += 8
            data = stream[offset:offset+data_len]
            offset += data_len
        elif ref_id == DeltaItem.REF_SELF:
            data = struct.unpack("!Q", stream[offset:offset+8])[0]
            offset += 8
        elif ref_id == DeltaItem.REF_BASE_DISK or \
                ref_id == DeltaItem.REF_BASE_MEM:
            data = struct.unpack("!Q", stream[offset:offset+8])[0]
            offset += 8
        elif ref_id == DeltaItem.REF_SELF_HASH:
            # print "unpacking ref_self_hash"
            data = struct.unpack("!32s", stream[offset:offset+32])[0]
            offset += 32

        if delta_type == DeltaItem.DELTA_DISK_LIVE or\
                delta_type == DeltaItem.DELTA_MEMORY_LIVE:
            live_seq = struct.unpack("!H", stream[offset:offset+2])[0]
            offset += 2

        # hash_value typically does not exist when recovered becuase we don't
        # need it
        if with_hashvalue:
            # hash_value is only needed for residue case
            hash_value = struct.unpack("!32s", stream[offset:offset+32])[0]
            offset += 32
            item = DeltaItem(delta_type, ram_offset, offset_len,
                             hash_value, ref_id, data_len,
                             live_seq=live_seq)
        else:
            item = DeltaItem(delta_type, ram_offset, offset_len,
                             None, ref_id, data_len, data,
                             live_seq=live_seq)
        return item, offset

    def process_deltaitem(self, delta_item, overlay_chunk_ids):
        if len(delta_item.data) != delta_item.offset_len:
            msg = "recovered size is not same as page size, %ld != %ld" % \
                (len(delta_item.data), delta_item.offset_len)
            raise StreamSynthesisError(msg)

        # save it to dictionary to find self_reference easily
        self.recovered_delta_dict[delta_item.index] = delta_item
        self.recovered_hash_dict[delta_item.hash_value] = delta_item

        # do nothing if the latest memory or disk are already process
        prev_iter_item = self.live_migration_iteration_dict.get(
            delta_item.index)
        if (prev_iter_item is not None):
            prev_seq = getattr(prev_iter_item, 'live_seq', 0)
            item_seq = getattr(delta_item, 'live_seq', 0)
            if prev_seq > item_seq:
                msg = "Latest version is already synthesized at %d (%d)" % (
                    delta_item.offset, delta_item.delta_type)
                LOG.debug(msg)
                return

        # write to output file
        overlay_chunk_id = long(delta_item.offset/self.chunk_size)
        if delta_item.delta_type == DeltaItem.DELTA_MEMORY or\
                delta_item.delta_type == DeltaItem.DELTA_MEMORY_LIVE:
            self.recover_mem_fd.seek(delta_item.offset)
            self.recover_mem_fd.write(delta_item.data)
            overlay_chunk_ids.append(
                "%d:%ld" %
                (RecoverDeltaProc.FUSE_INDEX_MEMORY, overlay_chunk_id))
        elif delta_item.delta_type == DeltaItem.DELTA_DISK or\
                delta_item.delta_type == DeltaItem.DELTA_DISK_LIVE:
            self.recover_disk_fd.seek(delta_item.offset)
            self.recover_disk_fd.write(delta_item.data)
            overlay_chunk_ids.append(
                "%d:%ld" %
                (RecoverDeltaProc.FUSE_INDEX_DISK, overlay_chunk_id))

        # update the latest item for each memory page or disk block
        self.live_migration_iteration_dict[delta_item.index] = delta_item

    def finish(self):
        self.recovered_delta_dict.clear()
        self.recovered_delta_dict = None
        self.recovered_hash_dict.clear()
        self.recovered_hash_dict = None
        self.live_migration_iteration_dict.clear()
        self.live_migration_iteration_dict = None
        if self.base_disk_fd is not None:
            self.base_disk_fd.close()
            self.base_disk_fd = None
        if self.base_mem_fd is not None:
            self.base_mem_fd.close()
            self.base_mem_fd = None
        if self.raw_disk is not None:
            self.raw_disk.close()
            self.raw_disk = None
        if self.raw_mem is not None:
            self.raw_mem.close()
            self.raw_mem = None
        if self.raw_mem_overlay is not None:
            self.raw_mem_overlay.close()
            self.raw_mem_overlay = None


class StreamSynthesisHandler(SocketServer.StreamRequestHandler):
    synthesis_option = {
        Protocol.SYNTHESIS_OPTION_DISPLAY_VNC: False,
        Protocol.SYNTHESIS_OPTION_EARLY_START: False,
        Protocol.SYNTHESIS_OPTION_SHOW_STATISTICS: False
        }

    def ret_fail(self, message):
        LOG.error("%s" % str(message))
        message = NetworkUtil.encoding({
            Protocol.KEY_COMMAND: Protocol.MESSAGE_COMMAND_FAIELD,
            Protocol.KEY_FAILED_REASON: message
            })
        message_size = struct.pack("!I", len(message))
        self.request.send(message_size)
        self.wfile.write(message)

    def ret_success(self, req_command, payload=None):
        send_message = {
            Protocol.KEY_COMMAND: Protocol.MESSAGE_COMMAND_SUCCESS,
            Protocol.KEY_REQUESTED_COMMAND: req_command,
            }
        if payload:
            send_message.update(payload)
        message = NetworkUtil.encoding(send_message)
        message_size = struct.pack("!I", len(message))
        self.request.send(message_size)
        self.wfile.write(message)
        self.wfile.flush()

    def send_synthesis_done(self):
        message = NetworkUtil.encoding({
            Protocol.KEY_COMMAND: Protocol.MESSAGE_COMMAND_SYNTHESIS_DONE,
            })
        LOG.info("SUCCESS to launch VM")
        try:
            message_size = struct.pack("!I", len(message))
            self.request.send(message_size)
            self.wfile.write(message)
        except socket.error as e:
            pass

    def _recv_all(self, recv_size, ack_size=1024*1024):
        prev_ack_sent_size = 0
        data = ''
        while len(data) < recv_size:
            tmp_data = self.request.recv(recv_size-len(data))
            if tmp_data is None:
                msg = "Cannot recv data at %s" % str(self)
                raise StreamSynthesisError(msg)
            if len(tmp_data) == 0:
                raise StreamSynthesisError("Recv 0 data at %s" % str(self))
            data += tmp_data

            # to send ack for every PERIODIC_ACK_BYTES bytes
            cur_recv_size = len(data)
            data_diff = cur_recv_size-prev_ack_sent_size
            if data_diff > ack_size or cur_recv_size >= recv_size:
                ack_data = struct.pack("!Q", data_diff)
                self.request.sendall(ack_data)
                prev_ack_sent_size = cur_recv_size
        return data

    def _check_validity(self, message):
        header_info = None
        requested_base = None

        synthesis_option = message.get(Protocol.KEY_SYNTHESIS_OPTION, None)
        base_hashvalue = message.get(Cloudlet_Const.META_BASE_VM_SHA256, None)

        # check base VM
        for each_basevm in self.server.basevm_list:
            if base_hashvalue == each_basevm['hash_value']:
                LOG.info(
                    "New client request %s VM" %
                    (each_basevm['diskpath']))
                requested_base = each_basevm['diskpath']
        return [synthesis_option, requested_base]

    def handle(self):
        '''Handle request from the client
        Each request follows this format:

        | header size | header | blob header size | blob header | blob data  |
        |  (4 bytes)  | (var)  | (4 bytes)        | (var bytes) | (var bytes)|
        '''
        # variable
        self.total_recved_size_cur = 0
        self.total_recved_size_prev = 0

        # get header
        data = self._recv_all(4)
        if data is None or len(data) != 4:
            raise StreamSynthesisError(
                "Failed to receive first byte of header")
        message_size = struct.unpack("!I", data)[0]
        msgpack_data = self._recv_all(message_size)
        metadata = NetworkUtil.decoding(msgpack_data)
        launch_disk_size = metadata[Cloudlet_Const.META_RESUME_VM_DISK_SIZE]
        launch_memory_size = metadata[Cloudlet_Const.META_RESUME_VM_MEMORY_SIZE]

        synthesis_option, base_diskpath = self._check_validity(metadata)
        if base_diskpath is None:
            raise StreamSynthesisError("No matching base VM")
        base_diskpath, base_mempath, base_diskmeta, base_memmeta =\
            self.server.handoff_data.base_vm_paths
        LOG.info("  - %s" % str(pformat(self.synthesis_option)))
        LOG.info("  - Base VM     : %s" % base_diskpath)

        # variables for FUSE
        launch_disk = self.server.handoff_data.launch_diskpath
        launch_mem = self.server.handoff_data.launch_memorypath
        memory_chunk_all = set()
        disk_chunk_all = set()

        # start pipelining processes
        network_out_queue = multiprocessing.Queue()
        decomp_queue = multiprocessing.Queue()
        fuse_info_queue = multiprocessing.Queue()
        decomp_proc = DecompProc(network_out_queue, decomp_queue, num_proc=4)
        decomp_proc.start()
        LOG.info("Start Decompression process")
        delta_proc = RecoverDeltaProc(base_diskpath, base_mempath,
                                      decomp_queue,
                                      launch_mem,
                                      launch_disk,
                                      Cloudlet_Const.CHUNK_SIZE,
                                      fuse_info_queue)
        delta_proc.start()
        LOG.info("Start Synthesis process")

        # get each blob
        recv_blob_counter = 0
        while True:
            data = self._recv_all(4)
            if data is None or len(data) != 4:
                msg = "Failed to receive first byte of header"
                raise StreamSynthesisError(msg)
                break
            blob_header_size = struct.unpack("!I", data)[0]
            blob_header_raw = self._recv_all(blob_header_size)
            blob_header = NetworkUtil.decoding(blob_header_raw)
            blob_size = blob_header.get(Cloudlet_Const.META_OVERLAY_FILE_SIZE)
            if blob_size is None:
                raise StreamSynthesisError("Failed to receive blob")
            if blob_size == 0:
                LOG.debug("%f\tend of stream" % (time.time()))
                break
            blob_comp_type = blob_header.get(
                Cloudlet_Const.META_OVERLAY_FILE_COMPRESSION)
            blob_disk_chunk = blob_header.get(
                Cloudlet_Const.META_OVERLAY_FILE_DISK_CHUNKS)
            blob_memory_chunk = blob_header.get(
                Cloudlet_Const.META_OVERLAY_FILE_MEMORY_CHUNKS)

            # send ack right before getting the blob
            ack_data = struct.pack("!Q", 0x01)
            self.request.send(ack_data)
            compressed_blob = self._recv_all(blob_size, ack_size=200*1024)
            # send ack right after getting the blob
            ack_data = struct.pack("!Q", 0x02)
            self.request.send(ack_data)

            network_out_queue.put((blob_comp_type, compressed_blob))
            memory_chunk_set = set(
                ["%ld:1" % item for item in blob_memory_chunk])
            disk_chunk_set = set(["%ld:1" % item for item in blob_disk_chunk])
            memory_chunk_all.update(memory_chunk_set)
            disk_chunk_all.update(disk_chunk_set)
            LOG.debug("%f\treceive one blob" % (time.time()))
            recv_blob_counter += 1

        network_out_queue.put(Cloudlet_Const.QUEUE_SUCCESS_MESSAGE)
        delta_proc.join()
        LOG.debug("%f\tdeltaproc join" % (time.time()))

        # send end message
        actual_resume_time = time.time()
        ack_data = struct.pack("!Qd", 0x10, actual_resume_time)
        LOG.debug("send ack to client: %d" % len(ack_data))
        self.request.sendall(ack_data)
        LOG.info("finished")

        disk_overlay_map = ','.join(disk_chunk_all)
        memory_overlay_map = ','.join(memory_chunk_all)
        sys.stdout.write("openstack\t%s\t%s\t%s\t%s" % (
                         launch_disk_size, launch_memory_size,
                         disk_overlay_map, memory_overlay_map))

    def terminate(self):
        # force terminate when something wrong in handling request
        # do not wait for joinining
        if hasattr(self, 'delta_proc') and self.delta_proc is not None:
            self.delta_proc.finish()
            if self.delta_proc.is_alive():
                self.delta_proc.terminate()
            self.delta_proc = None


class StreamSynthesisConst(object):
    SERVER_PORT_NUMBER = 8022
    VERSION = 0.1


class StreamSynthesisServer(SocketServer.TCPServer):

    def __init__(self, handoff_datafile,
                 port_number=StreamSynthesisConst.SERVER_PORT_NUMBER,
                 timeout=120):
        self._handoff_datafile = handoff_datafile
        self.port_number = port_number
        self.timeout = timeout
        self.handoff_data = self._load_handoff_data(self._handoff_datafile)
        self.basevm_list = self.check_basevm(
            self.handoff_data.base_vm_paths,
            self.handoff_data.basevm_sha256_hash
        )
        server_address = ("0.0.0.0", self.port_number)
        self.allow_reuse_address = True
        try:
            SocketServer.TCPServer.__init__(
                self,
                server_address,
                StreamSynthesisHandler)
        except socket.error as e:
            LOG.error(str(e))
            LOG.error("Check IP/Port : %s\n" % (str(server_address)))
            sys.exit(1)
        self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
        LOG.info("* Server configuration")
        LOG.info(" - Open TCP Server at %s" % (str(server_address)))
        LOG.info(" - Time out for waiting: %d" % (self.timeout))
        LOG.info(" - Disable Nagle(No TCP delay)  : %s" %
                 str(self.socket.getsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY)))
        LOG.info("-"*50)

    def _load_handoff_data(self, filepath):
        handoff_data = HandoffDataRecv.from_file(filepath)
        if handoff_data is None:
            raise StreamSynthesisError(
                "Invalid handoff recv data at %s" % filepath)
        LOG.info("Load handoff data file at %s" % filepath)
        return handoff_data

    def handle_error(self, request, client_address):
        SocketServer.TCPServer.handle_error(self, request, client_address)
        LOG.error("handling error from client %s\n" % (str(client_address)))
        LOG.error(traceback.format_exc())
        LOG.error("%s" % str(e))

    def handle_timeout(self):
        LOG.error("timeout error\n")

    def terminate(self):
        # close all thread
        if self.socket != -1:
            self.socket.close()

        global session_resources
        for (session_id, resource) in session_resources.iteritems():
            try:
                resource.deallocate()
            except Exception as e:
                msg = "Failed to deallocate resources for Session : %s" % str(
                    session_id)
                LOG.warning(msg)

    def check_basevm(self, base_vm_paths, hash_value):
        ret_list = list()
        LOG.info("-"*50)
        LOG.info("* Base VM Configuration")
        # check file location
        (base_diskpath,
         base_mempath,
         base_diskmeta,
         base_memmeta) = base_vm_paths
        if not os.path.exists(base_diskpath):
            LOG.warning("base disk is not available at %s" % base_diskpath)
        if not os.path.exists(base_mempath):
            LOG.warning("base memory is not available at %s" % base_mempath)
        if not os.path.exists(base_diskmeta):
            LOG.warning("disk hashlist is not available at %s" % base_diskmeta)
        if not os.path.exists(base_memmeta):
            LOG.warning(
                "memory hashlist is not available at %s" %
                base_memmeta)
        basevm_item = {'hash_value': hash_value, 'diskpath': base_diskpath}
        ret_list.append(basevm_item)

        LOG.info("  %s (Disk %d MB, Memory %d MB)" %
                (base_diskpath, os.path.getsize(base_diskpath)/1024/1024,
                 os.path.getsize(base_mempath)/1024/1024))
        LOG.info("-"*50)
        return ret_list


def sigint_handler(signum, frame):
    sys.stdout.write("Exit by user\n")
    if server is not None:
        server.terminate()
    sys.exit(0)


def main(argv=sys.argv):
    if not validate_congifuration():
        sys.stderr.write("failed to validate configuration\n")
        sys.exit(1)

    parser = OptionParser(usage="usage: %prog ")
    parser.add_option("-p", "--port", action="store", dest="port_number",
                      default=StreamSynthesisConst.SERVER_PORT_NUMBER,
                      help="port number for handoff")
    parser.add_option("-d", "--datafile", action="store",
                      dest="handoff_datafile", default=None,
                      help="specify datafile for handoff destination")
    settings, args = parser.parse_args(argv)
    if settings.handoff_datafile is None:
        sys.stderr.write("Need to specify path to the handoff datafile\n")
        sys.exit(1)
    settings.handoff_datafile = os.path.abspath(settings.handoff_datafile)

    server = StreamSynthesisServer(
        settings.handoff_datafile,
        int(settings.port_number), timeout=120,
    )
    try:
        server.handle_request()
    except Exception as e:
        # sys.stderr.write(str(e))
        server.terminate()
        sys.exit(1)
    except KeyboardInterrupt as e:
        sys.stdout.write("Exit by user\n")
        server.terminate()
        sys.exit(1)
    else:
        server.terminate()
        sys.exit(0)


if __name__ == "__main__":
    main()
